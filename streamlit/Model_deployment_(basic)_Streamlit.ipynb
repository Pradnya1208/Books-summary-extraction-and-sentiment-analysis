{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pradnya1208/Books-summary-extraction-and-sentiment-analysis/blob/main/streamlit/Model_deployment_(basic)_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F62QE5OYLBPt"
      },
      "source": [
        "#Exploratory Data Analysis on Book Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Auf-fNjllY3I",
        "outputId": "e26d3cc2-3b5d-4c67-fa16-0aac187406ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "\n",
        "import string\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "06vcydgjliju"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "wordnet = WordNetLemmatizer()\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LrTDZP2wNL4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb4cf17-8096-4eaa-e036-c50c5bf076bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 10.1 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 42.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 111 kB 65.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 72.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 164 kB 67.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 130 kB 62.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 793 kB 62.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 380 kB 49.9 MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.10.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.32.0 which is incompatible.\u001b[0m\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit --quiet\n",
        "!pip install pyngrok==4.1.1 --quiet\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nrclex import NRCLex\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "wordnet = WordNetLemmatizer()\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf = TfidfVectorizer()\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import string\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import collections\n",
        "from textblob import TextBlob\n",
        "import sys"
      ],
      "metadata": {
        "id": "_Ht1WwhirB6e"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit"
      ],
      "metadata": {
        "id": "39v9ybuYFlDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5xvSAI1oq8yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install NRCLex"
      ],
      "metadata": {
        "id": "zXRiiNV1iBnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0da6505c-d289-428e-97f5-b0d41441a2da"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting NRCLex\n",
            "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 92 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 327 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 337 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 348 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 358 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 368 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 378 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 389 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 396 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from NRCLex) (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob->NRCLex) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob->NRCLex) (1.15.0)\n",
            "Building wheels for collected packages: NRCLex\n",
            "  Building wheel for NRCLex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43329 sha256=6746a554a91112e146d2b2946df508f46b6511095539ee749ae62f7afcdd7457\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/2c/9c/dfa19d1b65326c520b32850a9311f6d4eda679ac04dba26081\n",
            "Successfully built NRCLex\n",
            "Installing collected packages: NRCLex\n",
            "Successfully installed NRCLex-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Mar 13 13:16:44 2022\n",
        "\n",
        "@author: Pradnya Patil\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "#from tracemalloc import stop\n",
        "#!pip install NRCLex\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import nltk\n",
        "from nrclex import NRCLex\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "wordnet = WordNetLemmatizer()\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf = TfidfVectorizer()\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import string\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import collections\n",
        "from textblob import TextBlob\n",
        "import sys\n",
        "\n",
        "# For DL Model\n",
        "import keras\n",
        "from keras.preprocessing import text, sequence\n",
        "from nltk.tokenize import word_tokenize\n",
        "max_words = 15000\n",
        "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
        "model_dl = keras.models.load_model('/content/model')\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "with open(\"negative-words.txt\",\"r\", encoding='latin-1') as neg:\n",
        "    negwords = neg.read().split(\"\\n\")\n",
        "\n",
        "with open(\"positive-words.txt\",\"r\") as pos:\n",
        "    poswords = pos.read().split(\"\\n\")\n",
        "\n",
        "with open(\"afinn2.txt\",\"r\") as affin:\n",
        "    affinity = affin.read().split(\"\\n\")\n",
        "\n",
        "affinity_data = pd.read_csv('afinn2.txt', sep=\"\\t\", header=None, names=[\"word\", \"value\"])\n",
        "affinity_scores = affinity_data.set_index('word')['value'].to_dict()\n",
        "sentiment_lexicon = affinity_scores\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "#st1, st2, st3 = st.columns((15,1,1))\n",
        "\n",
        "\n",
        "#def local_css(filename):\n",
        "#  with open(filename) as f:\n",
        "#    st1.caption('<style>{}</style>'.format(f.read()), unsafe_allow_html=True)\n",
        "\n",
        "#local_css(\"/content/style.css\")\n",
        "\n",
        "#st.title(\"Hello world\")\n",
        "\n",
        "\n",
        "#st.title('Summary Extraction with Sentiment Analysis')\n",
        "st.markdown(\"<h2 style='text-align: center; color: black;'>Summary Extraction with Sentiment Analysis</h2>\", unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "# Create a page dropdown \n",
        "page = st.selectbox(\"Choose your page\", [\"Page 1\", \"Page 2\", \"Page 3\"]) \n",
        "if page == \"Page 1\":\n",
        "    # Display details of page 1\n",
        "\n",
        "\n",
        "def preprocess_summary(summary_dataframe):\n",
        "  filtered_sum=[]\n",
        "  filtered_sent=[]\n",
        "  summary = [x.strip() for x in summary_dataframe]\n",
        "\n",
        "  for i in range(len(summary)):\n",
        "    summary_ = re.sub(\"[^A-Za-z\" \"]+\",\" \",summary[i])\n",
        "    summary_ = re.sub(\"[0-9\" \"]+\",\" \",summary[i])\n",
        "    \n",
        "    summary_ = summary_.lower()\n",
        "    summary_ =summary_.split()\n",
        "    summary_ = [wordnet.lemmatize(word) for word in summary_ if not word in set(stopwords.words('english'))]\n",
        "    summary_ = ' '.join(summary_)\n",
        "    filtered_sum.append(summary_)\n",
        "    text_tf = tf.fit_transform(filtered_sum)\n",
        "    feature_names = tf.get_feature_names()\n",
        "    dense = text_tf.todense()\n",
        "    denselist = dense.tolist()\n",
        "    summary_df =pd.DataFrame(denselist, columns=feature_names)\n",
        "    \n",
        "    return summary_df, filtered_sum\n",
        "\n",
        "\n",
        "def word_cloud(summary_df):\n",
        "  cloud = ' '.join(summary_df)\n",
        "\n",
        "  f, axes = plt.subplots(figsize=(10,5))\n",
        "  wordcloud= WordCloud(\n",
        "        background_color = 'white',\n",
        "        width = 1800,\n",
        "        height =1400).generate(cloud)\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad=100)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def word_cloud_positive(summary_df):\n",
        "  f, axes = plt.subplots(figsize=(10,5))\n",
        "  pos_words = ' '.join([w for w in summary_df if w in poswords])\n",
        "\n",
        "  cloud_pos = WordCloud(\n",
        "        background_color = 'white',\n",
        "        width =1800,\n",
        "        height=1400).generate(pos_words)\n",
        "  plt.imshow(cloud_pos)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def word_cloud_negative(summary_df):\n",
        "  f, axes = plt.subplots(figsize=(10,5))\n",
        "  neg_words = ' '.join([w for w in summary_df if w in negwords])\n",
        "\n",
        "  cloud_neg = WordCloud(\n",
        "        background_color='white',\n",
        "        width =1800,\n",
        "        height =1400).generate(neg_words)\n",
        "  plt.imshow(cloud_neg)\n",
        "\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # Parts of speech distribution Analysis\n",
        "def get_pos_tags(sentences, tagset='universal'):\n",
        "  #Create the Dataframe to store the count of tags\n",
        "  df = pd.DataFrame(columns=['ADJ','ADP','ADV','CONJ','DET','NOUN','NUM','PRT','PRON','VERB','.','X'])\n",
        "  for sent in sentences:\n",
        "      # Extract the part of Speech tags in the sentence\n",
        "      pos_tags = Counter([j for i,j in nltk.pos_tag(word_tokenize(sent), tagset='universal')])\n",
        "      #Appends the pos tags to the dataframe, fill NaN values with 0\n",
        "      df = df.append(pos_tags, ignore_index=True).fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "  fig = plt.figure(figsize =(10, 7))\n",
        "  data = df[0:1].values[0]\n",
        "  col = df.columns.values\n",
        " \n",
        "  # Horizontal Bar Plot\n",
        "  plt.bar(col, data)\n",
        " \n",
        "  # Show Plot\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "def calc_subj(sum_):\n",
        "  subj = TextBlob(sum_).sentiment.subjectivity\n",
        "  polar = TextBlob(sum_).sentiment.polarity\n",
        "  \n",
        "  return subj, polar\n",
        "\n",
        "def emotion_score(summary_em):\n",
        "  \n",
        "  \n",
        "  anger=[];disgust=[];fear=[];joy=[];surprise=[];trust=[];anticipation=[];sadness=[];positive=[];negative=[]\n",
        "  emotions= [\"anger\",\"disgust\",\"fear\",\"joy\",\"surprise\",\"trust\",\"anticipation\",\"sadness\",\"positive\",\"negative\"]\n",
        "\n",
        "  \n",
        "  emotion = NRCLex(summary_em)\n",
        "\n",
        "  if \"positive\" in emotion.raw_emotion_scores.keys():\n",
        "    positive.append(emotion.raw_emotion_scores['positive'])\n",
        "  else:\n",
        "    positive.append(0)\n",
        "\n",
        "\n",
        "  if \"anger\" in emotion.raw_emotion_scores.keys():\n",
        "    anger.append(emotion.raw_emotion_scores['anger'])\n",
        "  else:\n",
        "    anger.append(0)\n",
        "\n",
        "  if \"disgust\" in emotion.raw_emotion_scores.keys():\n",
        "    disgust.append(emotion.raw_emotion_scores['disgust'])\n",
        "  else:\n",
        "    disgust.append(0)\n",
        "\n",
        "  if \"fear\" in emotion.raw_emotion_scores.keys():\n",
        "    fear.append(emotion.raw_emotion_scores['fear'])\n",
        "  else:\n",
        "    fear.append(0)\n",
        "\n",
        "  if \"joy\" in emotion.raw_emotion_scores.keys():\n",
        "    joy.append(emotion.raw_emotion_scores['joy'])\n",
        "  else:\n",
        "    joy.append(0)\n",
        "\n",
        "  if \"surprise\" in emotion.raw_emotion_scores.keys():\n",
        "    surprise.append(emotion.raw_emotion_scores['surprise'])\n",
        "  else:\n",
        "    surprise.append(0)\n",
        "\n",
        "  if \"trust\" in emotion.raw_emotion_scores.keys():\n",
        "    trust.append(emotion.raw_emotion_scores['trust'])\n",
        "  else:\n",
        "    trust.append(0)\n",
        "\n",
        "  if \"anticipation\" in emotion.raw_emotion_scores.keys():\n",
        "    anticipation.append(emotion.raw_emotion_scores['anticipation'])\n",
        "  else:\n",
        "    anticipation.append(0)\n",
        "\n",
        "  if \"sadness\" in emotion.raw_emotion_scores.keys():\n",
        "    sadness.append(emotion.raw_emotion_scores['sadness'])\n",
        "  else:\n",
        "    sadness.append(0)\n",
        "\n",
        "  if \"negative\" in emotion.raw_emotion_scores.keys():\n",
        "    negative.append(emotion.raw_emotion_scores['negative'])\n",
        "  else:\n",
        "    negative.append(0)\n",
        "  \n",
        "  emotions_df = pd.DataFrame(list(zip(anger, anticipation, disgust, fear, joy, negative,positive, sadness, surprise, trust)),\n",
        "               columns =['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative',\n",
        "       'positive', 'sadness', 'surprise', 'trust'])\n",
        "  \n",
        "  fig = plt.figure(figsize =(10, 7))\n",
        "  data = emotions_df[0:1].values[0]\n",
        "  col = emotions_df.columns.values\n",
        " \n",
        "  # Horizontal Bar Plot\n",
        "  plt.barh(col, data)\n",
        " \n",
        "  # Show Plot\n",
        "  plt.show()\n",
        "  \n",
        " \n",
        "\n",
        "def calculate_sentiment(text: str = None):\n",
        "    sent_score = 0\n",
        "    if text:\n",
        "        sentence = nlp(text)\n",
        "        for word in sentence:\n",
        "            sent_score += sentiment_lexicon.get(word.lemma_, 0)\n",
        "    return sent_score\n",
        "\n",
        "\n",
        "# DL Model Start\n",
        "def find_sentiment(summary_dataframe):\n",
        "  \n",
        "\n",
        "  filtered_summary=[]\n",
        "  summary = [x.strip() for x in summary_dataframe]\n",
        "\n",
        "  for i in range(len(summary)):\n",
        "    summary_ = re.sub(\"[^A-Za-z\" \"]+\",\" \",summary[i])\n",
        "    summary_ = re.sub(\"[0-9\" \"]+\",\" \",summary[i])\n",
        "    \n",
        "    summary_ = summary_.lower()\n",
        "    filtered_summary.append(summary_)\n",
        "    \n",
        "  summary_df =pd.DataFrame()\n",
        "  summary_df[\"summary\"] = filtered_summary\n",
        "  \n",
        "  input_summary = summary_df[0:1].summary.values\n",
        "\n",
        "  # Tokenize the input\n",
        "  tokenize.fit_on_texts(input_summary)\n",
        "  tokenized_text = tokenize.texts_to_matrix(input_summary)\n",
        "\n",
        "  scores = model_dl.predict(tokenized_text, verbose=1, batch_size=32)\n",
        "  num_ = np.where(scores[0] == max(scores[0]))[0][0]\n",
        "  if num_ == 0:\n",
        "    y_pred= \"Negative\"\n",
        "  elif num_==1:\n",
        "    y_pred= \"Neutral\"\n",
        "  else:\n",
        "    y_pred= \"Positive\"\n",
        "\n",
        " \n",
        "  return y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# DL Model End\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Category=  pd.read_csv(r\"/content/Category.csv\")\n",
        "# select Category of Book like Sport,Art,Science etc.\n",
        "choice = st.sidebar.selectbox('Select Category',Category['Category'])\n",
        "#url1 = 'https://www.goodreads.com/shelf/show/'+str(choice)\n",
        "def get_choice(url1):\n",
        "    \n",
        "    #selecting Book Title\n",
        "    try:\n",
        "        Book_Title = []\n",
        "        req = requests.get(url1)\n",
        "        content = bs(req.content,'html.parser')\n",
        "        book = content.find('div',class_ = 'elementList')\n",
        "        for each in book:\n",
        "            spec = each.find_all_next('a',class_ = 'bookTitle')\n",
        "            for i in spec:\n",
        "                Book_Title.append(i.text)\n",
        "        x = Book_Title[:50]\n",
        "        del Book_Title\n",
        "        \n",
        "        return x\n",
        "        \n",
        "\n",
        "    except:\n",
        "        return st.write(\"Please Check Your Internet Connection\")\n",
        "  \n",
        "\n",
        "        \n",
        "        \n",
        "Book_Title = get_choice('https://www.goodreads.com/shelf/show/'+str(choice))\n",
        "\n",
        "\n",
        "if Book_Title != None:\n",
        "    Book = st.sidebar.selectbox('Select Book',Book_Title)\n",
        "    #get Book Summary:\n",
        "    Book_urls = []\n",
        "    req = requests.get('https://www.goodreads.com/shelf/show/'+str(choice))\n",
        "    content = bs(req.content,'html.parser')\n",
        "    \n",
        "    Bookdetails = content.find_all('div', class_ = 'elementList')\n",
        "    for book in Bookdetails:\n",
        "        book_anchors = book.find('a')\n",
        "        Book_url = 'https://www.goodreads.com' + book_anchors.get('href')\n",
        "        Book_urls.append(Book_url)\n",
        "    y = np.array(Book_urls[:50])\n",
        "    del Book_urls\n",
        "\n",
        "\n",
        "    for i,j in enumerate(Book_Title):\n",
        "        if j == str(Book):\n",
        "            url = y[i]\n",
        "            #st1.caption(url)\n",
        "            req = requests.get(url)\n",
        "            content = bs(req.content,'html.parser') \n",
        "            try:\n",
        "                summary_=\"\"\n",
        "                summary = content.find('div',class_ = 'readable stacked')\n",
        "                summary_ = summary.text\n",
        "                #st.write(summary_[:-8])\n",
        "                #st.info(summary_[:-8])\n",
        "                #book_summ = process_summary(summary_[:-8])\n",
        "                book_data_st = pd.DataFrame({'summary': [summary_[:-8]]}) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                #summary_df = preprocess_summary(book_data_st.summary)\n",
        "                \n",
        "          \n",
        "                with st.expander(\"Book Summary\"):\n",
        "                  st.info(summary_[:-8])\n",
        "\n",
        "                with st.expander(\"Book URL\"):\n",
        "                 \n",
        "                  st.caption(url)\n",
        "                  #st.table(book_data_st)\n",
        "\n",
        "                with st.expander(\"Most Common Words\"):\n",
        "                  summ_df,filtered_summary = preprocess_summary(book_data_st.summary)\n",
        "                  #st.table(preprocess_summary(summ_df))\n",
        "                  st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "                  st.pyplot(word_cloud(summ_df))\n",
        "                  \n",
        "\n",
        "                with st.expander(\"Positive Words\"):\n",
        "                  summ_df, filtered_summary= preprocess_summary(book_data_st.summary)\n",
        "                  #st.table(preprocess_summary(summ_df))\n",
        "                  st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "                  st.pyplot(word_cloud_positive(summ_df))\n",
        "\n",
        "                with st.expander(\"Negative Words\"):\n",
        "                  summ_df,filtered_summary = preprocess_summary(book_data_st.summary)\n",
        "                  #st.table(preprocess_summary(summ_df))\n",
        "                  st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "                  st.pyplot(word_cloud_negative(summ_df))\n",
        "\n",
        "                \n",
        "                with st.expander(\"Parts of Speech Distribution (POS)\"):\n",
        "                  summ_df,filtered_summary = preprocess_summary(book_data_st.summary)\n",
        "                  st.pyplot(get_pos_tags(filtered_summary))\n",
        "\n",
        "                with st.expander(\"Subjectivity and Polarity\"):\n",
        "                  st.write(calc_subj(book_data_st.summary.values[0]))\n",
        "\n",
        "\n",
        "\n",
        "                with st.expander(\"Emotion scores\"):\n",
        "                  st.pyplot(emotion_score(book_data_st.summary.values[0]))\n",
        "\n",
        "                \n",
        "\n",
        "                with st.expander(\"Sentiment Value\"):\n",
        "                  summ_df,filtered_summary = preprocess_summary(book_data_st.summary)\n",
        "                  st.write(calculate_sentiment(book_data_st.summary.values[0]))\n",
        "\n",
        "                with st.expander(\"Sentiment Analysis\"):\n",
        "                  st.write(find_sentiment(book_data_st.summary))\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "                 # books_data['word_count'] = books_data['filtered_summary'].str.split().apply(len)\n",
        "                \n",
        "\n",
        "\n",
        "                \n",
        "                  \n",
        "\n",
        "            except:\n",
        "                st.caption('Sorry We are Unable to Find Summary For this Book')    \n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCP5llm2GE72",
        "outputId": "eb0f3214-1909-4fad-bbea-e528c5d32613"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I5465tbgIvI9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(\"26lvckkeojDihziqJj00lMXnmcj_2wM7NMsyo2uwc5yqQs1m9\")"
      ],
      "metadata": {
        "id": "5t7EKJu6iquj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null&\n",
        "from pyngrok import ngrok\n",
        "\n",
        "url = ngrok.connect(port = '8501')\n",
        "print(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZG0g1pHGLPh",
        "outputId": "0f7a18f0-d71f-49be-ba61-fefaebe69a60"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://7d3b-35-243-196-157.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pgrep streamlit\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "F1w1AITninad"
      },
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Model deployment (basic)- Streamlit.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNs0i/lscZ2WeCmY3seiH7C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}