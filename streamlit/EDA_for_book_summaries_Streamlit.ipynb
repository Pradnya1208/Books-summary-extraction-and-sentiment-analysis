{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pradnya1208/Books-summary-extraction-and-sentiment-analysis/blob/main/streamlit/EDA_for_book_summaries_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F62QE5OYLBPt"
      },
      "source": [
        "#Exploratory Data Analysis on Book Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Auf-fNjllY3I",
        "outputId": "592e922c-fcb9-44d4-c7de-6f1489539c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "\n",
        "import string\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "06vcydgjliju"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "wordnet = WordNetLemmatizer()\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrTDZP2wNL4M"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit --quiet\n",
        "!pip install pyngrok==4.1.1 --quiet\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit"
      ],
      "metadata": {
        "id": "39v9ybuYFlDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install NRCLex"
      ],
      "metadata": {
        "id": "zXRiiNV1iBnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Mar 13 13:16:44 2022\n",
        "\n",
        "@author: Pradnya Patil\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "#from tracemalloc import stop\n",
        "#!pip install NRCLex\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import nltk\n",
        "from nrclex import NRCLex\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "wordnet = WordNetLemmatizer()\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf = TfidfVectorizer()\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import string\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import collections\n",
        "from textblob import TextBlob\n",
        "import sys\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "with open(\"negative-words.txt\",\"r\", encoding='latin-1') as neg:\n",
        "    negwords = neg.read().split(\"\\n\")\n",
        "\n",
        "with open(\"positive-words.txt\",\"r\") as pos:\n",
        "    poswords = pos.read().split(\"\\n\")\n",
        "\n",
        "with open(\"afinn2.txt\",\"r\") as affin:\n",
        "    affinity = affin.read().split(\"\\n\")\n",
        "\n",
        "affinity_data = pd.read_csv('afinn2.txt', sep=\"\\t\", header=None, names=[\"word\", \"value\"])\n",
        "affinity_scores = affinity_data.set_index('word')['value'].to_dict()\n",
        "sentiment_lexicon = affinity_scores\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "#st1, st2, st3 = st.columns((15,1,1))\n",
        "\n",
        "\n",
        "#def local_css(filename):\n",
        "#  with open(filename) as f:\n",
        "#    st1.caption('<style>{}</style>'.format(f.read()), unsafe_allow_html=True)\n",
        "\n",
        "#local_css(\"/content/style.css\")\n",
        "\n",
        "#st.title(\"Hello world\")\n",
        "\n",
        "\n",
        "#st.title('Summary Extraction with Sentiment Analysis')\n",
        "st.markdown(\"<h2 style='text-align: center; color: black;'>Summary Extraction with Sentiment Analysis</h2>\", unsafe_allow_html=True)\n",
        "\n",
        "def preprocess_summary(summary_dataframe):\n",
        "  filtered_sum=[]\n",
        "  filtered_sent=[]\n",
        "  summary = [x.strip() for x in summary_dataframe]\n",
        "\n",
        "  for i in range(len(summary)):\n",
        "    summary_ = re.sub(\"[^A-Za-z\" \"]+\",\" \",summary[i])\n",
        "    summary_ = re.sub(\"[0-9\" \"]+\",\" \",summary[i])\n",
        "    \n",
        "    summary_ = summary_.lower()\n",
        "    summary_ =summary_.split()\n",
        "    summary_ = [wordnet.lemmatize(word) for word in summary_ if not word in set(stopwords.words('english'))]\n",
        "    summary_ = ' '.join(summary_)\n",
        "    filtered_sum.append(summary_)\n",
        "    text_tf = tf.fit_transform(filtered_sum)\n",
        "    feature_names = tf.get_feature_names()\n",
        "    dense = text_tf.todense()\n",
        "    denselist = dense.tolist()\n",
        "    summary_df =pd.DataFrame(denselist, columns=feature_names)\n",
        "    \n",
        "    return summary_df, filtered_sum\n",
        "\n",
        "\n",
        "def word_cloud(summary_df):\n",
        "  cloud = ' '.join(summary_df)\n",
        "\n",
        "  f, axes = plt.subplots(figsize=(10,5))\n",
        "  wordcloud= WordCloud(\n",
        "        background_color = 'white',\n",
        "        width = 1800,\n",
        "        height =1400).generate(cloud)\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad=100)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def word_cloud_positive(summary_df):\n",
        "  f, axes = plt.subplots(figsize=(10,5))\n",
        "  pos_words = ' '.join([w for w in summary_df if w in poswords])\n",
        "\n",
        "  cloud_pos = WordCloud(\n",
        "        background_color = 'white',\n",
        "        width =1800,\n",
        "        height=1400).generate(pos_words)\n",
        "  plt.imshow(cloud_pos)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def word_cloud_negative(summary_df):\n",
        "  f, axes = plt.subplots(figsize=(10,5))\n",
        "  neg_words = ' '.join([w for w in summary_df if w in negwords])\n",
        "\n",
        "  cloud_neg = WordCloud(\n",
        "        background_color='white',\n",
        "        width =1800,\n",
        "        height =1400).generate(neg_words)\n",
        "  plt.imshow(cloud_neg)\n",
        "\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # Parts of speech distribution Analysis\n",
        "def get_pos_tags(sentences, tagset='universal'):\n",
        "  #Create the Dataframe to store the count of tags\n",
        "  df = pd.DataFrame(columns=['ADJ','ADP','ADV','CONJ','DET','NOUN','NUM','PRT','PRON','VERB','.','X'])\n",
        "  for sent in sentences:\n",
        "      # Extract the part of Speech tags in the sentence\n",
        "      pos_tags = Counter([j for i,j in nltk.pos_tag(word_tokenize(sent), tagset='universal')])\n",
        "      #Appends the pos tags to the dataframe, fill NaN values with 0\n",
        "      df = df.append(pos_tags, ignore_index=True).fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "  fig = plt.figure(figsize =(10, 7))\n",
        "  data = df[0:1].values[0]\n",
        "  col = df.columns.values\n",
        " \n",
        "  # Horizontal Bar Plot\n",
        "  plt.bar(col, data)\n",
        " \n",
        "  # Show Plot\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "def calc_subj(sum_):\n",
        "  subj = TextBlob(sum_).sentiment.subjectivity\n",
        "  polar = TextBlob(sum_).sentiment.polarity\n",
        "  \n",
        "  return subj, polar\n",
        "\n",
        "def emotion_score(summary_em):\n",
        "  \n",
        "  \n",
        "  anger=[];disgust=[];fear=[];joy=[];surprise=[];trust=[];anticipation=[];sadness=[];positive=[];negative=[]\n",
        "  emotions= [\"anger\",\"disgust\",\"fear\",\"joy\",\"surprise\",\"trust\",\"anticipation\",\"sadness\",\"positive\",\"negative\"]\n",
        "\n",
        "  \n",
        "  emotion = NRCLex(summary_em)\n",
        "\n",
        "  if \"positive\" in emotion.raw_emotion_scores.keys():\n",
        "    positive.append(emotion.raw_emotion_scores['positive'])\n",
        "  else:\n",
        "    positive.append(0)\n",
        "\n",
        "\n",
        "  if \"anger\" in emotion.raw_emotion_scores.keys():\n",
        "    anger.append(emotion.raw_emotion_scores['anger'])\n",
        "  else:\n",
        "    anger.append(0)\n",
        "\n",
        "  if \"disgust\" in emotion.raw_emotion_scores.keys():\n",
        "    disgust.append(emotion.raw_emotion_scores['disgust'])\n",
        "  else:\n",
        "    disgust.append(0)\n",
        "\n",
        "  if \"fear\" in emotion.raw_emotion_scores.keys():\n",
        "    fear.append(emotion.raw_emotion_scores['fear'])\n",
        "  else:\n",
        "    fear.append(0)\n",
        "\n",
        "  if \"joy\" in emotion.raw_emotion_scores.keys():\n",
        "    joy.append(emotion.raw_emotion_scores['joy'])\n",
        "  else:\n",
        "    joy.append(0)\n",
        "\n",
        "  if \"surprise\" in emotion.raw_emotion_scores.keys():\n",
        "    surprise.append(emotion.raw_emotion_scores['surprise'])\n",
        "  else:\n",
        "    surprise.append(0)\n",
        "\n",
        "  if \"trust\" in emotion.raw_emotion_scores.keys():\n",
        "    trust.append(emotion.raw_emotion_scores['trust'])\n",
        "  else:\n",
        "    trust.append(0)\n",
        "\n",
        "  if \"anticipation\" in emotion.raw_emotion_scores.keys():\n",
        "    anticipation.append(emotion.raw_emotion_scores['anticipation'])\n",
        "  else:\n",
        "    anticipation.append(0)\n",
        "\n",
        "  if \"sadness\" in emotion.raw_emotion_scores.keys():\n",
        "    sadness.append(emotion.raw_emotion_scores['sadness'])\n",
        "  else:\n",
        "    sadness.append(0)\n",
        "\n",
        "  if \"negative\" in emotion.raw_emotion_scores.keys():\n",
        "    negative.append(emotion.raw_emotion_scores['negative'])\n",
        "  else:\n",
        "    negative.append(0)\n",
        "  \n",
        "  emotions_df = pd.DataFrame(list(zip(anger, anticipation, disgust, fear, joy, negative,positive, sadness, surprise, trust)),\n",
        "               columns =['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative',\n",
        "       'positive', 'sadness', 'surprise', 'trust'])\n",
        "  \n",
        "  fig = plt.figure(figsize =(10, 7))\n",
        "  data = emotions_df[0:1].values[0]\n",
        "  col = emotions_df.columns.values\n",
        " \n",
        "  # Horizontal Bar Plot\n",
        "  plt.barh(col, data)\n",
        " \n",
        "  # Show Plot\n",
        "  plt.show()\n",
        "  \n",
        " \n",
        "\n",
        "def calculate_sentiment(text: str = None):\n",
        "    sent_score = 0\n",
        "    if text:\n",
        "        sentence = nlp(text)\n",
        "        for word in sentence:\n",
        "            sent_score += sentiment_lexicon.get(word.lemma_, 0)\n",
        "    return sent_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Category=  pd.read_csv(r\"/content/Category.csv\")\n",
        "# select Category of Book like Sport,Art,Science etc.\n",
        "choice = st.sidebar.selectbox('Select Category',Category['Category'])\n",
        "#url1 = 'https://www.goodreads.com/shelf/show/'+str(choice)\n",
        "def get_choice(url1):\n",
        "    \n",
        "    #selecting Book Title\n",
        "    try:\n",
        "        Book_Title = []\n",
        "        req = requests.get(url1)\n",
        "        content = bs(req.content,'html.parser')\n",
        "        book = content.find('div',class_ = 'elementList')\n",
        "        for each in book:\n",
        "            spec = each.find_all_next('a',class_ = 'bookTitle')\n",
        "            for i in spec:\n",
        "                Book_Title.append(i.text)\n",
        "        x = Book_Title[:50]\n",
        "        del Book_Title\n",
        "        \n",
        "        return x\n",
        "        \n",
        "\n",
        "    except:\n",
        "        return st.write(\"Please Check Your Internet Connection\")\n",
        "  \n",
        "\n",
        "        \n",
        "        \n",
        "Book_Title = get_choice('https://www.goodreads.com/shelf/show/'+str(choice))\n",
        "\n",
        "\n",
        "if Book_Title != None:\n",
        "    Book = st.sidebar.selectbox('Select Book',Book_Title)\n",
        "    #get Book Summary:\n",
        "    Book_urls = []\n",
        "    req = requests.get('https://www.goodreads.com/shelf/show/'+str(choice))\n",
        "    content = bs(req.content,'html.parser')\n",
        "    \n",
        "    Bookdetails = content.find_all('div', class_ = 'elementList')\n",
        "    for book in Bookdetails:\n",
        "        book_anchors = book.find('a')\n",
        "        Book_url = 'https://www.goodreads.com' + book_anchors.get('href')\n",
        "        Book_urls.append(Book_url)\n",
        "    y = np.array(Book_urls[:50])\n",
        "    del Book_urls\n",
        "\n",
        "\n",
        "    for i,j in enumerate(Book_Title):\n",
        "        if j == str(Book):\n",
        "            url = y[i]\n",
        "            #st1.caption(url)\n",
        "            req = requests.get(url)\n",
        "            content = bs(req.content,'html.parser') \n",
        "            try:\n",
        "                summary_=\"\"\n",
        "                summary = content.find('div',class_ = 'readable stacked')\n",
        "                summary_ = summary.text\n",
        "                #st.write(summary_[:-8])\n",
        "                #st.info(summary_[:-8])\n",
        "                #book_summ = process_summary(summary_[:-8])\n",
        "                book_data_st = pd.DataFrame({'summary': [summary_[:-8]]}) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                #summary_df = preprocess_summary(book_data_st.summary)\n",
        "                \n",
        "          \n",
        "                with st.expander(\"Book Summary\"):\n",
        "                  st.info(summary_[:-8])\n",
        "\n",
        "                with st.expander(\"Book URL\"):\n",
        "                 \n",
        "                  st.caption(url)\n",
        "                  #st.table(book_data_st)\n",
        "\n",
        "                with st.expander(\"Most Common Words\"):\n",
        "                  summ_df,filtered_summary = preprocess_summary(book_data_st.summary)\n",
        "                  #st.table(preprocess_summary(summ_df))\n",
        "                  st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "                  st.pyplot(word_cloud(summ_df))\n",
        "                  \n",
        "\n",
        "                with st.expander(\"Positive Words\"):\n",
        "                  summ_df, filtered_summary= preprocess_summary(book_data_st.summary)\n",
        "                  #st.table(preprocess_summary(summ_df))\n",
        "                  st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "                  st.pyplot(word_cloud_positive(summ_df))\n",
        "\n",
        "                with st.expander(\"Negative Words\"):\n",
        "                  summ_df,filtered_summary = preprocess_summary(book_data_st.summary)\n",
        "                  #st.table(preprocess_summary(summ_df))\n",
        "                  st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "                  st.pyplot(word_cloud_negative(summ_df))\n",
        "\n",
        "                \n",
        "                with st.expander(\"Parts of Speech Distribution (POS)\"):\n",
        "                  summ_df,filtered_summary = preprocess_summary(book_data_st.summary)\n",
        "                  st.pyplot(get_pos_tags(filtered_summary))\n",
        "\n",
        "                with st.expander(\"Subjectivity and Polarity\"):\n",
        "                  st.write(calc_subj(book_data_st.summary.values[0]))\n",
        "\n",
        "\n",
        "\n",
        "                with st.expander(\"Emotion scores\"):\n",
        "                  st.pyplot(emotion_score(book_data_st.summary.values[0]))\n",
        "\n",
        "                \n",
        "\n",
        "                with st.expander(\"Sentiment Value\"):\n",
        "                  summ_df,filtered_summary = preprocess_summary(book_data_st.summary)\n",
        "                  st.write(calculate_sentiment(book_data_st.summary.values[0]))\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "                 # books_data['word_count'] = books_data['filtered_summary'].str.split().apply(len)\n",
        "                \n",
        "\n",
        "\n",
        "                \n",
        "                  \n",
        "\n",
        "            except:\n",
        "                st.caption('Sorry We are Unable to Find Summary For this Book')    \n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCP5llm2GE72",
        "outputId": "08923a3d-228e-419f-ab03-be075a445f66"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import conf, ngrok\n",
        "ngrok.set_auth_token(\"26lvckkeojDihziqJj00lMXnmcj_2wM7NMsyo2uwc5yqQs1m9\")\n",
        "\n"
      ],
      "metadata": {
        "id": "I5465tbgIvI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null&\n",
        "from pyngrok import ngrok\n",
        "\n",
        "url = ngrok.connect(port = '8501')\n",
        "print(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZG0g1pHGLPh",
        "outputId": "e62840dd-f824-44cb-b01a-4d54016fba61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://777a-34-86-125-7.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pgrep streamlit\n",
        "#ngrok.kill()"
      ],
      "metadata": {
        "id": "MP6AR93wIaR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SV7vae3cI0Gb",
        "outputId": "97ab8ac6-7ea6-471f-8e59-318b9f1a9be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cl4ABRFVMS1w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EDA for book summaries - Streamlit.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO52eLFu8g8T7//nQxktZUY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}